from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.models import Variable
from airflow.utils.log.logging_mixin import LoggingMixin

from datetime import datetime, timedelta
import logging
from typing import List, Dict, Any
import os
from dotenv import load_dotenv

from airflow.decorators import dag, task
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.mysql.hooks.mysql import MySqlHook

from kaggle.api.kaggle_api_extended import KaggleApi
from sqlalchemy import create_engine
import pandas as pd
import json
import matplotlib.pyplot as plt
import numpy as np

# Load environment variables
load_dotenv()

# Configure logging
logger = logging.getLogger(__name__)

# Configuration
CONFIG = {
    'dataset_name': os.getenv('KAGGLE_DATASET_NAME', 'lokeshparab/amazon-products-dataset'),
    'download_path': os.getenv('DOWNLOAD_PATH', 'data'),
    'file_name': os.getenv('FILE_NAME', 'Amazon-Products.csv'),
    'relevant_columns': [
        "name", "main_category", "sub_category", 
        "discount_price", "actual_price", "ratings", "no_of_ratings"
    ],
    'invalid_ratings': ['nan', 'Get', 'FREE', '₹68.99', '₹65', '₹70', '₹100', '₹99', '₹2.99'],
    'currency_conversion_rate': float(os.getenv('CURRENCY_CONVERSION_RATE', '0.012')),
    'columns_to_check' : ["name","discount_price", "actual_price", "ratings", "no_of_ratings"],
    'database': {
        'name': os.getenv('DB_NAME', 'amazon_products'),
        'table': os.getenv('DB_TABLE', 'sales_data')
    }
}

default_args = {
    "owner": "airflow",
    "start_date": datetime(2025, 1, 1),
    "email": [os.getenv('AIRFLOW_EMAIL', 'airflow@example.com')],
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": int(os.getenv('AIRFLOW_RETRIES', '3')),
    "retry_delay": timedelta(minutes=int(os.getenv('AIRFLOW_RETRY_DELAY', '10')))
}

@dag(
    dag_id='amazon_products_etl',
    default_args=default_args,
    schedule="@once",
    catchup=False,
    tags=['project']
)
def amazon_products_etl():
    @task
    def download_kaggle_dataset(dataset_name: str, download_path: str) -> str:
        try:
            os.makedirs(download_path, exist_ok=True)
            api = KaggleApi()
            api.authenticate()
            api.dataset_download_files(dataset_name, path=download_path, unzip=True)

            csv_file_path = os.path.join(download_path, CONFIG['file_name'])
            
            # Read and process in chunks to handle large files
            chunks = pd.read_csv(csv_file_path, chunksize=10000)
            processed_chunks = []
            
            for chunk in chunks:
                processed_chunk = chunk[CONFIG['relevant_columns']]
                processed_chunks.append(processed_chunk)
            
            # Combine all chunks
            extracted_df = pd.concat(processed_chunks, ignore_index=True)
            extracted_df.to_csv(csv_file_path, index=False)
            
            logger.info(f"Successfully downloaded and processed dataset to {csv_file_path}")
            return csv_file_path
            
        except Exception as e:
            logger.error(f"Error downloading dataset: {str(e)}")
            raise

    @task
    def data_preprocessing(csv_file_path: str) -> str:
        try:
            # Read data in chunks
            chunks = pd.read_csv(csv_file_path, chunksize=10000)
            processed_chunks = []
            
            for chunk in chunks:

                chunk = chunk.drop_duplicates(subset=columns_to_check, keep="first")

                # Drop rows with missing values
                chunk.dropna(subset=['ratings', 'no_of_ratings', 'discount_price', 'actual_price'], inplace=True)
                
                # Drop invalid ratings
                for invalid_rating in CONFIG['invalid_ratings']:
                    chunk.drop(chunk[chunk['ratings'] == invalid_rating].index, inplace=True)
                
                # Process prices
                for price_col in ['discount_price', 'actual_price']:
                    chunk[price_col] = (chunk[price_col]
                                      .str.replace('₹', '', regex=False)
                                      .str.replace(',', '', regex=False)
                                      .astype(float)
                                      * CONFIG['currency_conversion_rate'])
                
                # Clean up ratings
                chunk['ratings'] = pd.to_numeric(chunk['ratings'], errors='coerce')
                chunk['no_of_ratings'] = chunk['no_of_ratings'].str.replace(',', '').astype(float)
                
                # Clean up text columns
                for col in ['name', 'main_category', 'sub_category']:
                    chunk[col] = chunk[col].str.strip()
                    chunk[col] = chunk[col].str.replace("'", "''")  # Escape single quotes

                # Add ID column
                chunk['id'] = range(1, len(chunk) + 1)

                processed_chunks.append(chunk)
            
            # Combine all processed chunks
            final_df = pd.concat(processed_chunks, ignore_index=True)
            
            # Drop any remaining rows with NaN values
            final_df = final_df.dropna()
            
            # Save the processed data
            final_df.to_csv(csv_file_path, index=False)
            
            logger.info(f"Successfully preprocessed data and saved to {csv_file_path}")
            return csv_file_path
            
        except Exception as e:
            logger.error(f"Error preprocessing data: {str(e)}")
            raise

    @task
    def upload_to_mysql(csv_file_path: str) -> None:
        try:
            # Get database connection details from Airflow connection
            hook = MySqlHook(mysql_conn_id='amazon_products_mysql')
            conn = hook.get_connection('amazon_products_mysql')
            
            # Create engine with database name included
            engine = create_engine(
                f'mysql+pymysql://{conn.login}:{conn.password}@{conn.host}:{conn.port}/{CONFIG["database"]["name"]}'
            )
            
            # Use a single connection for all operations
            with engine.connect() as connection:
                # Create database with proper permissions
                create_db_sql = f"""
                CREATE DATABASE IF NOT EXISTS {CONFIG['database']['name']}
                CHARACTER SET utf8mb4
                COLLATE utf8mb4_unicode_ci;
                """
                connection.execute(create_db_sql)
                logger.info(f"Created database {CONFIG['database']['name']}")
                
                # Grant permissions
                grant_sql = f"""
                GRANT ALL PRIVILEGES ON {CONFIG['database']['name']}.* TO 'airflow'@'%';
                FLUSH PRIVILEGES;
                """
                connection.execute(grant_sql)
                logger.info("Granted permissions")
                
                # Use the database
                connection.execute(f"USE {CONFIG['database']['name']};")
                logger.info(f"Using database {CONFIG['database']['name']}")
                
                # Drop table if exists to ensure clean state
                drop_table_sql = f"DROP TABLE IF EXISTS {CONFIG['database']['table']};"
                connection.execute(drop_table_sql)
                logger.info(f"Dropped existing table {CONFIG['database']['table']}")
                
                # Create table with fully qualified name and matching column names
                create_table_sql = f"""
                CREATE TABLE {CONFIG['database']['table']} (
                    id INTEGER PRIMARY KEY,
                    name VARCHAR(500) NOT NULL,
                    main_category VARCHAR(100) NOT NULL,
                    sub_category VARCHAR(100) NOT NULL,
                    discount_price FLOAT NOT NULL,
                    actual_price FLOAT NOT NULL,
                    ratings FLOAT NOT NULL,
                    no_of_ratings INTEGER NOT NULL
                );
                """
                connection.execute(create_table_sql)
                logger.info(f"Created table {CONFIG['database']['table']}")
                
                # Read and prepare data
                df = pd.read_csv(csv_file_path)
                logger.info(f"Read CSV file with {len(df)} rows")
                
                # Ensure column names match
                df = df.rename(columns={
                    'name': 'name',
                    'main_category': 'main_category',
                    'sub_category': 'sub_category',
                    'discount_price': 'discount_price',
                    'actual_price': 'actual_price',
                    'ratings': 'ratings',
                    'no_of_ratings': 'no_of_ratings'
                })
                
                # Verify table structure
                result = connection.execute(f"DESCRIBE {CONFIG['database']['table']}")
                columns = [row[0] for row in result]
                logger.info(f"Table columns: {columns}")
                
                # Upload data in chunks
                chunk_size = 10000
                total_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size else 0)
                
                for i, chunk in enumerate(np.array_split(df, total_chunks)):
                    # Ensure data types match
                    chunk['ratings'] = chunk['ratings'].astype(float)
                    chunk['no_of_ratings'] = chunk['no_of_ratings'].astype(float)
                    chunk['discount_price'] = chunk['discount_price'].astype(float)
                    chunk['actual_price'] = chunk['actual_price'].astype(float)
                    
                    chunk.to_sql(
                        name=CONFIG['database']['table'],
                        con=connection,
                        if_exists='append',
                        index=False
                    )
                    logger.info(f"Uploaded chunk {i+1}/{total_chunks}")
            
            logger.info(f"Successfully uploaded data to MySQL database")
            
        except Exception as e:
            logger.error(f"Error uploading to MySQL: {str(e)}")
            raise

    # Define task dependencies
    csv_file_path = download_kaggle_dataset(CONFIG['dataset_name'], CONFIG['download_path'])
    csv_file_path = data_preprocessing(csv_file_path)
    upload_to_mysql(csv_file_path)

amazon_products_dag = amazon_products_etl() 
